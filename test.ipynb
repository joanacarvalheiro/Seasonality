{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('boa tarde!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joana\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joana\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 20.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.5 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 24.8 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.6/12.6 MB 60.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 52.8 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.1 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\joana\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Downloading psycopg2-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.0/1.2 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 8.3 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2\n",
    "%pip show psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela criada (ou já existia).\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Configurações de conexão\n",
    "db_config = {\n",
    "    \"dbname\": \"Test\",\n",
    "    \"user\": \"avnadmin\",\n",
    "    \"password\": \"AVNS_9fZb3BkX9qGXxKpxsrZ\",\n",
    "    \"host\": \"postgresql-iscac.f.aivencloud.com\",  # ou o IP do servidor\n",
    "    \"port\": 25674,  # Porta padrão do PostgreSQL\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco de dados\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL para criar a tabela, se não existir\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tabela_test (\n",
    "        Id SERIAL PRIMARY KEY,\n",
    "        Name TEXT NOT NULL,\n",
    "        Age TEXT NOT NULL,\n",
    "        City TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "    # Confirmar a criação da tabela\n",
    "    conn.commit()\n",
    "    print(\"Tabela criada (ou já existia).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar tabela: {e}\")\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_api\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Carregar os dados da API em um dicionário Python\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m data_loc \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Normalizar os dados JSON para um DataFrame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_loc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(data_loc)\n",
      "File \u001b[1;32mc:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\joana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:356\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# URL da API\n",
    "URL = 'https://opendata.emel.pt/cycling/bicyclecounter/locations'\n",
    "\n",
    "# Fazer a requisição para a API\n",
    "response_api = requests.get(URL)\n",
    "print(response_api.status_code)\n",
    "\n",
    "# Carregar os dados da API em um dicionário Python\n",
    "data_loc = json.loads(response_api.text)\n",
    "\n",
    "# Normalizar os dados JSON para um DataFrame\n",
    "df_loc = pd.json_normalize(data_loc)\n",
    "\n",
    "# Mostrar as informações do DataFrame\n",
    "df_loc.info()\n",
    "\n",
    "# Configurações de conexão com o banco de dados PostgreSQL\n",
    "db_config = {\n",
    "    \"dbname\": \"Test\",\n",
    "    \"user\": \"avnadmin\",\n",
    "    \"password\": \"AVNS_9fZb3BkX9qGXxKpxsrZ\",\n",
    "    \"host\": \"postgresql-iscac.f.aivencloud.com\",  # ou o IP do servidor\n",
    "    \"port\": 25674,  # Porta padrão do PostgreSQL\n",
    "}\n",
    "\n",
    "# Função para criar a tabela, caso ela não exista\n",
    "def create_table(cursor):\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bicycle_locations (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        location_id INTEGER NOT NULL,\n",
    "        location_name TEXT NOT NULL,\n",
    "        latitude FLOAT,\n",
    "        longitude FLOAT,\n",
    "        last_updated TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "# Função para inserir ou atualizar os dados\n",
    "def insert_or_update_data(cursor, df):\n",
    "    for index, row in df.iterrows():\n",
    "        # Verificar se já existe um dado com o mesmo location_id\n",
    "        select_query = \"SELECT id FROM bicycle_locations WHERE location_id = %s\"\n",
    "        cursor.execute(select_query, (row['locationId'],))\n",
    "        existing_record = cursor.fetchone()\n",
    "\n",
    "        # Preparar os valores\n",
    "        location_id = row['locationId']\n",
    "        location_name = row['locationName']\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "        last_updated = row['lastUpdated']\n",
    "\n",
    "        # Se já existe, fazer um UPDATE\n",
    "        if existing_record:\n",
    "            update_query = \"\"\"\n",
    "            UPDATE bicycle_locations\n",
    "            SET location_name = %s, latitude = %s, longitude = %s, last_updated = %s\n",
    "            WHERE location_id = %s\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query, (location_name, latitude, longitude, last_updated, location_id))\n",
    "        else:\n",
    "            # Se não existe, fazer um INSERT\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO bicycle_locations (location_id, location_name, latitude, longitude, last_updated)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, (location_id, location_name, latitude, longitude, last_updated))\n",
    "\n",
    "# Função principal\n",
    "def main():\n",
    "    try:\n",
    "        # Conectar ao banco de dados\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Criar a tabela, caso não exista\n",
    "        create_table(cursor)\n",
    "\n",
    "        # Inserir ou atualizar os dados da API no banco de dados\n",
    "        insert_or_update_data(cursor, df_loc)\n",
    "\n",
    "        # Commit para salvar as alterações\n",
    "        conn.commit()\n",
    "        print(\"Dados inseridos ou atualizados com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Executar a função principal\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela tabela_joana criada com sucesso!\n",
      "Erro ao criar a tabela ou inserir os dados: duplicate key value violates unique constraint \"tabela_joana_pkey\"\n",
      "DETAIL:  Key (datetime)=(2021-10-22T00:00:00) already exists.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "db_config = {\n",
    "    \"dbname\": \"Test\",\n",
    "    \"user\": \"avnadmin\",\n",
    "    \"password\": \"AVNS_9fZb3BkX9qGXxKpxsrZ\",\n",
    "    \"host\": \"postgresql-iscac.f.aivencloud.com\",  # ou o IP do servidor\n",
    "    \"port\": 25674,  # Porta padrão do PostgreSQL\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file = \"test_joana.csv\"\n",
    "table_name = \"tabela_joana\"  # Nome da tabela que será criada\n",
    "\n",
    "def get_column_types(csv_file):\n",
    "    \"\"\"Lê as primeiras linhas do CSV e tenta inferir o tipo de dados das colunas.\"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        first_row = next(reader)\n",
    "        \n",
    "    column_types = {}\n",
    "    for col, value in first_row.items():\n",
    "        # Tentar inferir os tipos de dados das colunas (int, float, ou text)\n",
    "        try:\n",
    "            float(value)  # Se for numérico, considerar como FLOAT\n",
    "            column_types[col] = \"FLOAT\"\n",
    "        except ValueError:\n",
    "            column_types[col] = \"TEXT\"  # Caso contrário, é texto\n",
    "    return column_types\n",
    "\n",
    "def create_table(cursor, table_name, column_types):\n",
    "    \"\"\"Cria a tabela no PostgreSQL baseado nas colunas e seus tipos.\"\"\"\n",
    "    columns = f\"datetime TEXT PRIMARY KEY, \" + \", \".join([f\"{col} {col_type}\" for col, col_type in column_types.items() if col != \"datetime\"])\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns});\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "def insert_data_from_csv(cursor, csv_file, table_name, column_names):\n",
    "    \"\"\"Insere os dados do CSV na tabela.\"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            values = [row[col] if row[col] != '' else None for col in column_names]\n",
    "            insert_query = f\"INSERT INTO {table_name} ({', '.join(column_names)}) VALUES ({', '.join(['%s'] * len(values))});\"\n",
    "            cursor.execute(insert_query, values)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Conectar ao banco de dados\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Inferir os tipos de dados das colunas a partir do CSV\n",
    "        column_types = get_column_types(csv_file)\n",
    "\n",
    "        # Criar a tabela no banco de dados\n",
    "        create_table(cursor, table_name, column_types)\n",
    "        print(f\"Tabela {table_name} criada com sucesso!\")\n",
    "\n",
    "        # Inserir os dados do CSV na tabela\n",
    "        insert_data_from_csv(cursor, csv_file, table_name, column_types.keys())\n",
    "        conn.commit()\n",
    "        print(\"Dados inseridos com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar a tabela ou inserir os dados: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir a função Python que será chamada no workflow\n",
    "def print_hello():\n",
    "    print(\"Olá, este é um teste do Airflow!\")\n",
    "\n",
    "# Configurações padrão para o DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Definição do DAG\n",
    "dag = DAG(\n",
    "    'meu_workflow_print',  # Nome do DAG\n",
    "    default_args=default_args,\n",
    "    description='Um DAG simples para fazer um print',\n",
    "    schedule_interval='@daily',  # Executa diariamente\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    catchup=False  # Evita execuções retroativas\n",
    ")\n",
    "\n",
    "# Tarefa usando PythonOperator\n",
    "tarefa_print = PythonOperator(\n",
    "    task_id='print_teste',  # Identificador único da tarefa\n",
    "    python_callable=print_hello,  # Função Python a ser executada\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Fluxo de execução\n",
    "# Aqui só temos uma tarefa, mas outras poderiam ser adicionadas\n",
    "tarefa_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id    Name  Age      City\n",
      "0   1    John   30  New York\n",
      "1   2    Jane   25    London\n",
      "2   3  Rafael   28      None\n",
      "Dados inseridos com sucesso na tabela tabela_test.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    \"dbname\": \"Test\",\n",
    "    \"user\": \"avnadmin\",\n",
    "    \"password\": \"AVNS_9fZb3BkX9qGXxKpxsrZ\",\n",
    "    \"host\": \"postgresql-iscac.f.aivencloud.com\",  # ou o IP do servidor\n",
    "    \"port\": 25674,  # Porta padrão do PostgreSQL\n",
    "}\n",
    "\n",
    "# Caminho do arquivo CSV\n",
    "csv_file_path = \"test.csv\"\n",
    "\n",
    "# Nome da tabela no banco de dados\n",
    "table_name = \"tabela_test\"\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco de dados\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Lendo o arquivo CSV com pandas\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    print(df)\n",
    "\n",
    "    # Iterando pelas linhas do DataFrame para inserção\n",
    "    for _, row in df.iterrows():\n",
    "        placeholders = \", \".join([\"%s\"] * len(row))\n",
    "        columns = \", \".join(df.columns)\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "        cursor.execute(query, tuple(row))\n",
    "\n",
    "    # Confirmando as operações\n",
    "    conn.commit()\n",
    "    print(f\"Dados inseridos com sucesso na tabela {table_name}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "    conn.rollback()\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
